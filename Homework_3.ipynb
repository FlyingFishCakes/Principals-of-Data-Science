{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kevin Lin\n",
        "\n",
        "Data 602, Homework 3"
      ],
      "metadata": {
        "id": "UEyioaB84sAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Implement 1NN with 5-fold cross validation and use PCA to reduce the dimensionality to 60."
      ],
      "metadata": {
        "id": "lfZ1ZYb64wyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "nvxZO2cD5k7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare and Preprocess the Data\n",
        "First import the images and turn them into 1 dimensional vectors and create a list of their corresponding labels from the file names."
      ],
      "metadata": {
        "id": "sGd8MTDh5OSl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj0z_Q-K4oaP",
        "outputId": "2c49c375-87bf-4564-acfe-69d5933a13b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: 400 \n",
            "Image Vector Length: 10304 \n",
            "Labels: 400\n",
            "Label: 25 | Image: [123 122 121 ...  84  92  94]\n"
          ]
        }
      ],
      "source": [
        "# Unzip the data set\n",
        "file_path = '/content/Face Data for Homework.zip'\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "# Remove the text file contained in the folder\n",
        "if os.path.exists('/content/ATT/README'):\n",
        "    os.remove('/content/ATT/README')\n",
        "\n",
        "# Read each image row by row and turn into a 1D list and then append to a 2D list of all the images for a subject\n",
        "# Also get the Label for the image from the filename (ie. '10_2.png' is label_count.png) and use the label as the index in the list of lists\n",
        "vector_images = []\n",
        "vector_labels = []\n",
        "for image_file in Path('/content/ATT').glob('*.png'):\n",
        "    vector_labels.append(int(image_file.stem.split('_')[0]))\n",
        "    with Image.open(image_file) as image:\n",
        "      vector_images.append(np.array(image).flatten())\n",
        "\n",
        "print(\"Images:\", len(vector_images), \"\\nImage Vector Length:\", len(vector_images[0]), \"\\nLabels:\", len(vector_labels))\n",
        "print(\"Label:\", vector_labels[0], \"| Image:\", vector_images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement 1NN with 5-Fold Cross-Validation (Stratified)\n",
        "We need to shuffle the images within their corresponding labels so there the same amount of training and testing images for all 40 subjects. This will give 8 photos for training and 2 testing photos per person for a total of 320 training photos and 80 testing photos per fold.\n",
        "\n",
        "---\n",
        "\n",
        "Then for each fold implement PCA using `numpy.linalg.eig` on the dataset to be used for kNN where k = 1.\n",
        "\n",
        "---\n",
        "\n",
        "Test and display results for each fold. Then aggregate and average the results at the end (we are asked for average prediction accuracy which is TP/(TP+FP))."
      ],
      "metadata": {
        "id": "KEtY9hRc5Uyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make our own kNN classifier\n",
        "def ONEnn_predict(X_train, y_train, X_test):\n",
        "    # Distance is Euclidean norm between test point and all of training set samples\n",
        "    distances = np.linalg.norm(X_train - X_test, axis=1)\n",
        "\n",
        "    # Sort the distances to find the lowest value/closest point\n",
        "    sorted_indices = np.argsort(distances)\n",
        "\n",
        "    # Get and return the label of the closest point\n",
        "    return y_train[sorted_indices[0]]"
      ],
      "metadata": {
        "id": "_8ECbZai6PjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement our own SVD to get the Principal Components\n",
        "def compute_svd(matrix):\n",
        "  # Use the shorter side and not the image size side\n",
        "  left_singular_values = matrix @ matrix.T\n",
        "\n",
        "  # Eigenvalue decomposition to get the principal components\n",
        "  eigenvalues, U = np.linalg.eig(left_singular_values)\n",
        "\n",
        "  # Map the eigenvectors back onto the 320 by 10304 matrix (This is so we can later transform 10304 columns into 60 columns)\n",
        "  eigenvectors = matrix.T @ U\n",
        "\n",
        "  # Normalize the V matrix\n",
        "  eigenfaces = eigenvectors / np.linalg.norm(eigenvectors, axis=0)\n",
        "\n",
        "  # Return the first 60 principal components\n",
        "  return eigenfaces[:, :60]"
      ],
      "metadata": {
        "id": "W_B6E5kfKqr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the builtin StratifiedKFold to divide the folds\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# Convert vector_images and vector_labels into numpy arrays\n",
        "vector_images = np.array(vector_images)\n",
        "vector_labels = np.array(vector_labels)\n",
        "\n",
        "# For each fold implement the Eigenfaces (PCA) Method, and then test on the testing dataset\n",
        "results = []\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(vector_images, vector_labels), 1):\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test = vector_images[train_index], vector_images[test_index]\n",
        "    y_train, y_test = vector_labels[train_index], vector_labels[test_index]\n",
        "\n",
        "    # Center the training and testing features on the mean of the training sample\n",
        "    X_train_mean = np.mean(X_train, axis=0)\n",
        "    X_train_centered = X_train - X_train_mean\n",
        "    X_test_centered = X_test - X_train_mean\n",
        "\n",
        "    # Perform PCA on the training data\n",
        "    # X_train is 320x10304 2D list, I will end up with 320 possible principal components after SVD and I choose the first 60 Principal Components\n",
        "    pca_60 = compute_svd(X_train_centered)\n",
        "\n",
        "    # Transform the training and testing data using the PCA components\n",
        "    # Apply the PCA components to the centered training and test features\n",
        "    X_train_pca = X_train_centered @ pca_60\n",
        "    X_test_pca = X_test_centered @ pca_60\n",
        "    # print(X_train_pca.shape, X_test_pca.shape)\n",
        "\n",
        "    # Loop through each transformed test point and their corresponding test label, tally the correct predictions\n",
        "    correct_predictions = 0\n",
        "    for test_point, test_label in zip(X_test_pca, y_test):\n",
        "        # Predict the label for the test point\n",
        "        y_pred = ONEnn_predict(X_train_pca, y_train, test_point)\n",
        "        # Check if the prediction is correct\n",
        "        if y_pred == test_label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate the accuracy for this fold (TP + FP I realize is just the length of 1 fold)\n",
        "    accuracy = correct_predictions / len(y_test)\n",
        "    results.append(accuracy)\n",
        "\n",
        "    # Display the results for each of the folds\n",
        "    print(f\"Fold {fold}: Accuracy = {accuracy}\")\n",
        "\n",
        "\n",
        "# Aggregate the results at the end\n",
        "average_accuracy = np.mean(results)\n",
        "print(f\"Average Accuracy: {average_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPfhTYVI6306",
        "outputId": "d88772da-3db5-4ec8-ab07-8f969d795fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(320, 60) (80, 60)\n",
            "Fold 1: Accuracy = 0.975\n",
            "(320, 60) (80, 60)\n",
            "Fold 2: Accuracy = 0.9625\n",
            "(320, 60) (80, 60)\n",
            "Fold 3: Accuracy = 1.0\n",
            "(320, 60) (80, 60)\n",
            "Fold 4: Accuracy = 0.9625\n",
            "(320, 60) (80, 60)\n",
            "Fold 5: Accuracy = 0.975\n",
            "Average Accuracy: 0.975\n"
          ]
        }
      ]
    }
  ]
}